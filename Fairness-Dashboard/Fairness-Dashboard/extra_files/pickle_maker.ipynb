{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder \n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../flask_ml/data/Income_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target mapping used (Auto-detected positive class ' >50K'): {' <=50K': 0, ' >50K': 1}\n",
      "Train shape: (31258, 10), Validation shape: (7815, 10), Test shape: (9769, 10)\n",
      "Training the model pipeline...\n",
      "Training complete.\n",
      "Validation Accuracy: 0.8518\n",
      "Test Accuracy: 0.8504\n",
      "Number of features after preprocessing: 92\n",
      "Model pipeline saved to 'income_prediction_pipeline.pkl'\n",
      "Metadata saved to 'model_pipeline_metadata.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\debru004\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:246: UserWarning: Found unknown categories in columns [5] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "target_variable = 'Income'\n",
    "# Select the 10 features the model should *actually* be trained on\n",
    "# Match these with the user selection in the failing run:\n",
    "features_to_include = [\n",
    "    'workclass', 'education', 'education-num', 'marital-status', 'occupation',\n",
    "    'relationship', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country'\n",
    "]\n",
    "# Define original numeric and categorical columns *within the included features*\n",
    "numeric_columns = df[features_to_include].select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_columns = df[features_to_include].select_dtypes(exclude=np.number).columns.tolist()\n",
    "\n",
    "# All columns needed from the original CSV for processing\n",
    "columns_to_load = features_to_include + [target_variable]\n",
    "df_processed = df[columns_to_load].copy()\n",
    "\n",
    "# --- Target Encoding ---\n",
    "# Ensure target is binary 0/1\n",
    "unique_target = df_processed[target_variable].unique()\n",
    "if len(unique_target) > 2:\n",
    "    print(f\"Warning: Target variable '{target_variable}' has more than 2 values. Attempting mapping.\")\n",
    "    # Example mapping (adjust if needed) - assumes common income format\n",
    "    positive_label = '>50K' # Or find dynamically\n",
    "    target_map = {val: 1 if positive_label in str(val) else 0 for val in unique_target}\n",
    "    print(f\"Target mapping used: {target_map}\")\n",
    "else:\n",
    "    # Find the positive class label (e.g., ' >50K') assuming it's the less frequent one or specified\n",
    "    value_counts = df_processed[target_variable].value_counts()\n",
    "    positive_label = value_counts.idxmin() if len(value_counts) > 1 else unique_target[0]\n",
    "    target_map = {val: 1 if val == positive_label else 0 for val in unique_target}\n",
    "    print(f\"Target mapping used (Auto-detected positive class '{positive_label}'): {target_map}\")\n",
    "\n",
    "df_processed[target_variable] = df_processed[target_variable].map(target_map)\n",
    "\n",
    "# Separate Features (X) and Target (y)\n",
    "X_orig = df_processed[features_to_include]\n",
    "y = df_processed[target_variable]\n",
    "\n",
    "# --- Train/Validation/Test Split (on original features) ---\n",
    "# Split data: first 80% train+val, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_orig, y, test_size=0.20, random_state=42, stratify=y)\n",
    "\n",
    "# Split train+val into 80% train (64% of total), 20% validation (16% of total)\n",
    "# Test size for this split is 0.20 *within the train_val set* = 16% of total\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.20, random_state=42, stratify=y_train_val)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}, Validation shape: {X_val.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# --- Define Preprocessing Steps ---\n",
    "# Pipeline for numeric features: Impute missing values (median) then Scale\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler()) # Scaler applied ONLY to numeric features\n",
    "])\n",
    "\n",
    "# Pipeline for categorical features: Impute missing values (most frequent) then OneHotEncode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)) # drop='first' matches previous logic\n",
    "])\n",
    "\n",
    "# Create the ColumnTransformer\n",
    "# This applies the transformers to the correct columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ],\n",
    "    remainder='passthrough' \n",
    ")\n",
    "\n",
    "# --- Create the Full Model Pipeline ---\n",
    "# Chain the preprocessor and the logistic regression model\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# --- Train the Full Pipeline ---\n",
    "print(\"Training the model pipeline...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- Evaluate ---\n",
    "# Evaluate on validation set\n",
    "y_val_pred = model_pipeline.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_test_pred = model_pipeline.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# --- Get Final Feature Names AFTER Preprocessing ---\n",
    "# This is important for the metadata and consistency checks\n",
    "try:\n",
    "    final_feature_names = model_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "    final_feature_names = [name.split('__')[-1] for name in final_feature_names]\n",
    "    print(f\"Number of features after preprocessing: {len(final_feature_names)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not get feature names from ColumnTransformer: {e}\")\n",
    "    final_feature_names = [\"error_getting_names\"] # Placeholder\n",
    "\n",
    "\n",
    "# --- Save Model and Preprocessor ---\n",
    "# Save the *entire pipeline* which includes the preprocessor and the classifier\n",
    "model_path = 'income_prediction_pipeline.pkl' # Changed name to reflect it's a pipeline\n",
    "with open(model_path, 'wb') as file:\n",
    "    pickle.dump(model_pipeline, file)\n",
    "print(f\"Model pipeline saved to '{model_path}'\")\n",
    "\n",
    "# --- Create and Save Metadata ---\n",
    "# Metadata should now clearly list the *original* features expected as input\n",
    "# and the final features *after* the saved preprocessor runs.\n",
    "metadata = {\n",
    "    \"model_info\": {\n",
    "        \"pipeline_file\": model_path, # Changed key name\n",
    "        \"target_variable\": target_variable,\n",
    "        \"model_type_in_pipeline\": type(model_pipeline.named_steps['classifier']).__name__, # e.g., LogisticRegression\n",
    "        \"creation_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"validation_accuracy\": float(val_accuracy),\n",
    "        \"test_accuracy\": float(test_accuracy)\n",
    "    },\n",
    "    \"feature_info\": {\n",
    "        # Features EXPECTED AS INPUT to the pipeline\n",
    "        \"input_features_ordered\": features_to_include, # Order matters if not using ColumnTransformer by name\n",
    "        \"input_numeric_features\": numeric_columns,\n",
    "        \"input_categorical_features\": categorical_columns,\n",
    "        \"input_feature_count\": len(features_to_include),\n",
    "        # Features AFTER the pipeline's preprocessor step runs\n",
    "        \"processed_feature_names_ordered\": final_feature_names,\n",
    "        \"processed_feature_count\": len(final_feature_names)\n",
    "    },\n",
    "    \"preprocessing_in_pipeline\": {\n",
    "        \"numeric_strategy\": \"Impute Median -> StandardScaler\",\n",
    "        \"categorical_strategy\": \"Impute Mode -> OneHotEncoder (drop='first')\",\n",
    "        # Details extracted from the pipeline components if needed\n",
    "    },\n",
    "     \"evaluation_split\": { # Added info about the split used for validation/test accuracy reported\n",
    "         \"train_percentage\": 0.64, # 80% of 80%\n",
    "         \"validation_percentage\": 0.16, # 20% of 80%\n",
    "         \"test_percentage\": 0.20,\n",
    "         \"random_state\": 42\n",
    "     }\n",
    "}\n",
    "\n",
    "metadata_path = 'model_pipeline_metadata.json' \n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(f\"Metadata saved to '{metadata_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved to 'model_metadata.json'\n"
     ]
    }
   ],
   "source": [
    "# Create and save metadata\n",
    "metadata = {\n",
    "    \"model_info\": {\n",
    "        \"target_variable\": \"Income\",\n",
    "        \"excluded_features\": columns_to_drop + [\"Income\"],\n",
    "        \"creation_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"model_file\": model_path,\n",
    "        \"validation_accuracy\": float(val_accuracy),\n",
    "        \"test_accuracy\": float(test_accuracy)\n",
    "    },\n",
    "    \"feature_info\": {\n",
    "        \"original_features\": all_original_features,\n",
    "        \"included_original_features\": included_original_features,\n",
    "        \"numeric_features\": numeric_columns,\n",
    "        \"categorical_features\": categorical_columns,\n",
    "        \"one_hot_encoded_features\": encoded_features,\n",
    "        \"final_feature_list\": X.columns.tolist(),\n",
    "        \"feature_count\": len(X.columns)\n",
    "    },\n",
    "    \"preprocessing\": {\n",
    "        \"scaling\": \"StandardScaler\",\n",
    "        \"missing_values\": \"numeric filled with median, remaining rows dropped\",\n",
    "        \"categorical_encoding\": \"one-hot with drop_first=True\",\n",
    "        \"train_val_test_split\": \"80:10:10\"\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(f\"Metadata saved to '{metadata_path}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
